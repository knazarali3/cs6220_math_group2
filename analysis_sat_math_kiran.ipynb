{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llema_file_path = \"inference_sat_math_results/llemma_math_sat_answers_2.csv\"\n",
    "\n",
    "mistral_file_path = \"inference_sat_math_results/Mistral_Inference_SAT_Dataset.csv\"\n",
    "\n",
    "llama_file_path = \"inference_sat_math_results/the_inference_llama3_sat_math.csv\"\n",
    "\n",
    "qwen_file_path = \"inference_sat_math_results/inference_QWEN_sat_math.csv\"\n",
    "\n",
    "def extract_numbers(s):\n",
    "    normalized = re.sub(r'[^\\d\\s]', '', s)  \n",
    "    return list(map(int, re.findall(r'\\d+', normalized))) \n",
    "def compare_number_lists(list1, list2):\n",
    "    return set(list1).issubset(set(list2))  # list1=correct solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "sat_math_dict = load_dataset(\"knazarali3/group2_processed_sat_math_cot\")\n",
    "train_dataset = sat_math_dict[\"train\"]\n",
    "test_dataset = sat_math_dict[\"test\"]\n",
    "test_dataset_df = pd.DataFrame(test_dataset)\n",
    "test_dataset_df[\"row_index\"] = test_dataset_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_llema_inference_sat_math():\n",
    "    df = pd.read_csv(llema_file_path, index_col=0)\n",
    "    llema_test_dataset= df.merge(test_dataset_df, left_on='Question Id', right_on='id')\n",
    "\n",
    "    llema_test_dataset['Correct Final Solution'] = llema_test_dataset['answer'].apply(extract_numbers)\n",
    "\n",
    "    llema_test_dataset[\"llema_is_correct\"] = llema_test_dataset.apply(\n",
    "        lambda row: row[\"Final Solution\"] in row[\"Correct Final Solution\"], axis=1\n",
    "    )\n",
    "\n",
    "    llema_test_dataset = llema_test_dataset.rename(columns={\"Final Solution\": \"llema_solution\"})\n",
    "\n",
    "    llema_3_results = llema_test_dataset[[\"Question Id\", \"llema_is_correct\", \"llema_solution\"]]\n",
    "    return llema_3_results\n",
    "\n",
    "\n",
    "def format_mistral_inference_sat_math():\n",
    "    df = pd.read_csv(mistral_file_path)\n",
    "    mistral_test_dataset= df.merge(test_dataset_df, left_on='ID', right_on='id')\n",
    "\n",
    "    mistral_test_dataset['Final Solution Numbers'] = mistral_test_dataset['answer'].apply(extract_numbers)\n",
    "\n",
    "\n",
    "    mistral_test_dataset['Run_1'] = mistral_test_dataset['Run_1'].astype(str)\n",
    "\n",
    "    mistral_test_dataset['Mistral Model Solution Numbers'] = mistral_test_dataset['Run_1'].apply(extract_numbers)\n",
    "\n",
    "    mistral_test_dataset['mistral_is_correct'] = [\n",
    "        compare_number_lists(a, b) for a, b in zip(mistral_test_dataset['Final Solution Numbers'], mistral_test_dataset['Mistral Model Solution Numbers'])\n",
    "    ]\n",
    "\n",
    "    mistral_test_dataset = mistral_test_dataset.rename(columns={\"ID\": \"Question Id\"})\n",
    "\n",
    "    mistral_results = mistral_test_dataset[[\"Question Id\", \"mistral_is_correct\", \"Mistral Model Solution Numbers\"]]\n",
    "    return mistral_results    \n",
    "\n",
    "def format_llama_inference_sat_math():\n",
    "    df = pd.read_csv(llama_file_path)\n",
    "    llama_test_dataset= df.merge(test_dataset_df, left_on='ID', right_on='id')\n",
    "\n",
    "    llama_test_dataset['Final Solution Numbers'] = llama_test_dataset['answer'].apply(extract_numbers)\n",
    "\n",
    "    llama_test_dataset['Run_1'] = llama_test_dataset['Run_1'].astype(str)\n",
    "\n",
    "    llama_test_dataset['Llama Model Solution Numbers'] = llama_test_dataset['Run_1'].apply(extract_numbers)\n",
    "\n",
    "    llama_test_dataset['llama_is_correct'] = [\n",
    "        compare_number_lists(a, b) for a, b in zip(llama_test_dataset['Final Solution Numbers'], llama_test_dataset['Llama Model Solution Numbers'])\n",
    "    ]\n",
    "\n",
    "    llama_test_dataset = llama_test_dataset.rename(columns={\"ID\": \"Question Id\"})\n",
    "\n",
    "    llama_results = llama_test_dataset[[\"Question Id\", \"llama_is_correct\", \"Llama Model Solution Numbers\"]]\n",
    "    return llama_results\n",
    "\n",
    "def format_qwen_inference_sat_math():\n",
    "    df = pd.read_csv(qwen_file_path)\n",
    "    qwen_test_dataset= df.merge(test_dataset_df, left_on='row_index', right_on='row_index')\n",
    "\n",
    "\n",
    "    qwen_test_dataset['Final Solution Numbers'] = qwen_test_dataset['answer'].apply(extract_numbers)\n",
    "\n",
    "    qwen_test_dataset['Run_1'] = qwen_test_dataset['Run_1'].astype(str)\n",
    "\n",
    "    qwen_test_dataset['Qwen Model Solution Numbers'] = qwen_test_dataset['Run_1'].apply(extract_numbers)\n",
    "\n",
    "    qwen_test_dataset['qwen_is_correct'] = [\n",
    "        compare_number_lists(a, b) for a, b in zip(qwen_test_dataset['Final Solution Numbers'], qwen_test_dataset['Qwen Model Solution Numbers'])\n",
    "    ]\n",
    "\n",
    "    qwen_test_dataset = qwen_test_dataset.rename(columns={\"id\": \"Question Id\"})\n",
    "\n",
    "    qwen_results = qwen_test_dataset[[\"Question Id\", \"qwen_is_correct\", \"Qwen Model Solution Numbers\"]]\n",
    "\n",
    "    return qwen_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_correct\n",
       "False    6042\n",
       "True      152\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_results = format_llama_inference_sat_math()\n",
    "llema_results = format_llema_inference_sat_math()\n",
    "qwen_results = format_qwen_inference_sat_math()\n",
    "mistral_results = format_mistral_inference_sat_math()\n",
    "\n",
    "# Check where all the answers correct column is True\n",
    "# i.e. all models got the answer correct\n",
    "all_results = ((llama_results.merge(llema_results)).merge(qwen_results)).merge(mistral_results)\n",
    "all_results[\"all_correct\"] = (all_results[\"llama_is_correct\"] == True) & (all_results[\"llema_is_correct\"] == True) & (all_results[\"qwen_is_correct\"] == True) & (all_results[\"mistral_is_correct\"]==True)\n",
    "\n",
    "all_results[\"all_correct\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_is_correct\n",
       "True     3862\n",
       "False    2428\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_results[\"llama_is_correct\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llema_is_correct\n",
       "False    5354\n",
       "True      936\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llema_results[\"llema_is_correct\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qwen_is_correct\n",
       "False    3156\n",
       "True     3134\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_results[\"qwen_is_correct\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mistral_is_correct\n",
       "False    5168\n",
       "True     1026\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_results[\"mistral_is_correct\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
