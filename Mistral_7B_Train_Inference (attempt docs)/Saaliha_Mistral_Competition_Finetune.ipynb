{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install transformers torch datasets peft trl pandas bitsandbytes"
      ],
      "metadata": {
        "id": "-4cZ1osaygCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "O6HB1nM63N6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_path = '/content/drive/My Drive/Outputs_Competition/mistral7b_inference_results.csv'"
      ],
      "metadata": {
        "id": "P6HIr_WVAhhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch, random\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "random.seed(42)\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"SaalihaA/mistral_v7_SAT_Math_Dataset\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "        token = \"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\"\n",
        "    )"
      ],
      "metadata": {
        "id": "Rnhu9gTi3Qqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "5aPoNqi-5oKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format Dataset and Prompt"
      ],
      "metadata": {
        "id": "RVm--vph4Jb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(data):\n",
        "    data_df = pd.DataFrame(data)\n",
        "    data_df['index'] = data_df.index + 1\n",
        "    final_answers = []\n",
        "\n",
        "    for _, row in data_df.iterrows():\n",
        "        chain = row[\"extracted_solution\"]\n",
        "        final_answers.append(chain)\n",
        "\n",
        "    data_df[\"final_answer\"] = final_answers\n",
        "    return Dataset.from_pandas(data_df)"
      ],
      "metadata": {
        "id": "y99yEaJW5dK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_format_dataset():\n",
        "    dataset = load_dataset(\"jeggers/competition_math\", \"original\")\n",
        "    train_data, test_data = dataset[\"train\"], dataset[\"test\"]\n",
        "    return format_dataset(train_data), format_dataset(test_data)"
      ],
      "metadata": {
        "id": "AmY2Mp1C5d3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_prompt_data(data, tokenizer, for_training=False):\n",
        "    instruction = \"\"\"Below is an instruction describing a task, paired with an input that provides context. Write a response that completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    Solve the math problem and provide only the final answer.\n",
        "\n",
        "    ### Input:\n",
        "    {}\n",
        "\n",
        "    ### Response:\n",
        "    {}\n",
        "    \"\"\"\n",
        "    EOS_TOKEN = tokenizer.eos_token  # Ensure EOS_TOKEN is correctly defined\n",
        "\n",
        "\n",
        "    def process(example):\n",
        "        if for_training:\n",
        "            prompt = instruction.format(example[\"question\"], example.get(\"final_answer\", \"\"))\n",
        "            return {\n",
        "                \"id\": example[\"index\"],  # Use 'id' for tracking in training\n",
        "                \"problem\": example[\"problem\"],  # The input question\n",
        "                \"actual_answer\": example[\"extracted_solution\"],  # The expected answer for training\n",
        "                \"text\": prompt + EOS_TOKEN\n",
        "            }\n",
        "        else:\n",
        "            prompt = instruction.format(example[\"question\"])\n",
        "            return {\n",
        "                \"id\": example[\"index\"],\n",
        "                \"question\": example[\"question\"],\n",
        "                \"text\": prompt + EOS_TOKEN\n",
        "            }\n",
        "\n",
        "    return data.map(process)\n"
      ],
      "metadata": {
        "id": "sDHGVq_e3wgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "BmoVWtbJ4SAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "output_dir = '/content/drive/My Drive/Outputs_Competition'\n",
        "# Load and format datasets\n",
        "train_dataset, test_dataset = load_and_format_dataset()\n",
        "train_data = prepare_prompt_data(train_dataset, tokenizer, for_training=True)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_data,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 10,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        #num_train_epochs = 1, # For longer training runs!\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = output_dir,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "6RWwLwbx_-hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "71WvflZ_AF1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "HlCl9OrM4U5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "results = []\n",
        "FastLanguageModel.for_inference(model)\n",
        "inference_dataset = prepare_prompt_data(test_dataset, tokenizer, for_training=False)\n",
        "print(\"inference dataset\", inference_dataset)\n",
        "# Tokenize the dataset for inference\n",
        "\n",
        "tokenized_inputs = tokenizer(\n",
        "    [example[\"text\"] for example in inference_dataset],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    padding_side='left',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "#print(inference_dataset)\n",
        "# Prepare batches\n",
        "batch_size = 15  # Adjust as needed based on GPU/CPU memory\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(0, len(inference_dataset), batch_size), desc=\"Inference Batches\"):\n",
        "    batch_questions = inference_dataset[\"text\"][i:i + batch_size]\n",
        "    batch_ids = inference_dataset[\"id\"][i:i + batch_size]\n",
        "    batch_correct_answers = inference_dataset[\"correct_answer\"][i:i + batch_size]\n",
        "    batch_levels = inference_dataset[\"level\"][i:i + batch_size]\n",
        "    batch_types = inference_dataset[\"type\"][i:i + batch_size]\n",
        "\n",
        "    inputs = tokenizer(batch_questions, return_tensors=\"pt\", padding=True, padding_side='left', truncation=True).to(\"cuda\")\n",
        "\n",
        "    # Tokenized inputs for the current batch\n",
        "    input_ids = tokenized_inputs[\"input_ids\"][i:i+batch_size]\n",
        "    attention_mask = tokenized_inputs[\"attention_mask\"][i:i+batch_size]\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            temperature=1e-5,  # For deterministic behavior\n",
        "            top_k=None,\n",
        "            top_p=None,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            max_time=100\n",
        "        )\n",
        "\n",
        "    batch_decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    print(len(batch_decoded_outputs))\n",
        "\n",
        "    # Process outputs and combine results\n",
        "    for qid, question, raw_output, correct_answer, level, qtype in zip(batch_ids, batch_raw_questions, outputs, batch_correct_answers, batch_levels, batch_types):\n",
        "        response_start = output.find(\"### Response:\") + len(\"### Response:\")\n",
        "        response = output[response_start:].strip()\n",
        "        response = response.replace(\"<|end_of_text|>\", \"\").strip()\n",
        "\n",
        "        try:\n",
        "            response_dict = ast.literal_eval(response)\n",
        "\n",
        "            if isinstance(response_dict, dict):\n",
        "                final_answer = response_dict.get(\"final_answer\", \"N/A\")\n",
        "            else:\n",
        "                final_answer = \"Error: Response is not a dictionary\"\n",
        "                print(f\"Raw response: {response}\")\n",
        "        except (ValueError, SyntaxError):\n",
        "            final_answer = \"Error Parsing Response\"\n",
        "\n",
        "        # Save results\n",
        "        results.append({\n",
        "            \"ID\": qid,\n",
        "            \"Problem\": question,\n",
        "            \"Level\": level,\n",
        "            \"Type\": qtype,\n",
        "            \"Output\": decoded_output,\n",
        "            \"Final Answer\": final_answer,\n",
        "            \"Correct Answer\": correct_answer\n",
        "        })\n",
        "\n",
        "# Save results to a CSV file\n",
        "#import pandas as pd\n",
        "#output_df = pd.DataFrame(results)\n",
        "#output_df.to_csv(output_path, index=False)\n",
        "#print(f\"Inference results saved to {output_path}\")"
      ],
      "metadata": {
        "id": "O0_G7Hu1lRkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "results = []\n",
        "FastLanguageModel.for_inference(model)\n",
        "inference_dataset = prepare_prompt_data(test_dataset, tokenizer, for_training=False)\n",
        "print(\"inference dataset\", inference_dataset)\n",
        "# Tokenize the dataset for inference\n",
        "\n",
        "tokenized_inputs = tokenizer(\n",
        "    [example[\"text\"] for example in inference_dataset],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    padding_side='left',\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "#print(inference_dataset)\n",
        "# Prepare batches\n",
        "batch_size = 15  # Adjust as needed based on GPU/CPU memory\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(0, len(inference_dataset), batch_size), desc=\"Inference Batches\"):\n",
        "    batch_questions = inference_dataset[\"text\"][i:i + batch_size]\n",
        "    batch_ids = inference_dataset[\"id\"][i:i + batch_size]\n",
        "    batch_raw_questions = inference_dataset[\"problem\"][i:i + batch_size]\n",
        "    batch_correct_answers = inference_dataset[\"correct_answer\"][i:i + batch_size]\n",
        "    batch_levels = inference_dataset[\"level\"][i:i + batch_size]\n",
        "    batch_types = inference_dataset[\"type\"][i:i + batch_size]\n",
        "\n",
        "    #inputs = tokenizer(batch_questions, return_tensors=\"pt\", padding=True, padding_side='left', truncation=True).to(\"cuda\")\n",
        "\n",
        "    # Tokenized inputs for the current batch\n",
        "    input_ids = tokenized_inputs[\"input_ids\"][i:i+batch_size]\n",
        "    attention_mask = tokenized_inputs[\"attention_mask\"][i:i+batch_size]\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model.generate(\n",
        "        #**inputs,\n",
        "        #max_new_tokens=50,\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=50  # Adjust as needed\n",
        "    )\n",
        "\n",
        "    # Process outputs and combine results\n",
        "    for qid, question, raw_output, correct_answer, level, qtype in zip(batch_ids, batch_raw_questions, outputs, batch_correct_answers, batch_levels, batch_types):\n",
        "        decoded_output = tokenizer.decode(raw_output, skip_special_tokens=True).strip()\n",
        "        #print(decoded_output)\n",
        "        # Extract the final answer\n",
        "        try:\n",
        "            # Find the starting position of the \"### Response:\" part\n",
        "            response_start = decoded_output.find(\"### Response:\")\n",
        "\n",
        "            if response_start != -1:  # Ensure the marker was found\n",
        "                response_start += len(\"### Response:\")\n",
        "                final_answer = decoded_output[response_start:].strip()\n",
        "                final_answer = final_answer.replace(\"<|end_of_text|>\", \"\").strip()\n",
        "            else:\n",
        "                final_answer = \"Error Parsing Response While Extracting\"  # If \"### Response:\" was not found\n",
        "            #print(\"final_answer\", final_answer)\n",
        "\n",
        "        except Exception as e:\n",
        "            final_answer = \"Error Parsing Response\"  # Handle unexpected errors\n",
        "\n",
        "        # Save results\n",
        "        results.append({\n",
        "            \"ID\": qid,\n",
        "            \"Problem\": question,\n",
        "            \"Level\": level,\n",
        "            \"Type\": qtype,\n",
        "            \"Output\": decoded_output,\n",
        "            \"Final Answer\": final_answer,\n",
        "            \"Correct Answer\": correct_answer\n",
        "        })\n",
        "\n",
        "# Save results to a CSV file\n",
        "#import pandas as pd\n",
        "#output_df = pd.DataFrame(results)\n",
        "#output_df.to_csv(output_path, index=False)\n",
        "#print(f\"Inference results saved to {output_path}\")"
      ],
      "metadata": {
        "id": "Lh0FxqCYANYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming this part of your code creates the DataFrame from results\n",
        "import pandas as pd\n",
        "output_df = pd.DataFrame(results)\n",
        "\n",
        "# Function to check if answers match\n",
        "def answers_match(final_answer, correct_answer):\n",
        "    # Convert both answers to lowercase and strip whitespace\n",
        "    final_answer = str(final_answer).lower().strip()\n",
        "    correct_answer = str(correct_answer).lower().strip()\n",
        "\n",
        "    # Remove any LaTeX formatting if present\n",
        "    final_answer = final_answer.replace('\\\\', '').replace('{', '').replace('}', '')\n",
        "    correct_answer = correct_answer.replace('\\\\', '').replace('{', '').replace('}', '')\n",
        "\n",
        "    return final_answer == correct_answer\n",
        "\n",
        "# Add the new column\n",
        "output_df['Is_Correct'] = output_df.apply(lambda row: answers_match(row['Final Answer'], row['Correct Answer']), axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = output_df['Is_Correct'].mean()\n",
        "print(f\"Model Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Save the updated DataFrame\n",
        "output_df.to_csv(output_path, index=False)\n",
        "print(f\"Updated inference results saved to {output_path}\")"
      ],
      "metadata": {
        "id": "8MrNoNznBEmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push to Hugginface"
      ],
      "metadata": {
        "id": "KZKCrzGw4bPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"SaalihaA/CS6220-Mistral_7b_Competition_Dataset\", token = \"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\") # Online saving\n",
        "print(\"Training complete!\")\n",
        "tokenizer.push_to_hub(\"SaalihaA/CS6220-Mistral_7b_Competition_Dataset\", token = \"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\") # Online saving\n",
        "print(\"Tokenizer uploaded!\")"
      ],
      "metadata": {
        "id": "nQxhWYQ7XK8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login() #(must be in a separate code block from the next lines)\n",
        "repo_name=\"SaalihaA/CS6220-Mistral_7b_Competition_Dataset\"\n",
        "model.push_to_hub(repo_name)\n",
        "tokenizer.push_to_hub(repo_name)"
      ],
      "metadata": {
        "id": "5VsciIi_QMBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}